# Structure Stage Migration Plan

## Overview

Replace current 8-phase structure stage with new 2-phase sliding window architecture.

**Goal:** Simpler, cheaper, more accurate structure detection with proper provenance tracking and semantic chunking for RAG.

**Strategy:** Clean cutover - delete old implementation, build new from scratch.

---

## Current State (v1) - TO BE DELETED

**Architecture:**
```
Phase 0: Load pages (filter by region type)
Phase 1-2: Structure detection (Claude Sonnet 4.5, full book)
Phase 3: Page number extraction (GPT-4o-mini, parallel)
Phase 6: Footnote extraction (GPT-4o-mini, chunks)
Phase 7: Bibliography parsing (GPT-4o-mini, chunks)
Phase 8: Output generation (Python)
```

**Issues:**
- âŒ Content loss: Filtering header regions loses ~25-30% of body text
- âŒ Expensive: $2.00 for full-book structure detection
- âŒ No RAG chunks: Treats full pages as "paragraphs"
- âŒ Confusing phase numbers (0, 1-2, 3, 6, 7, 8)
- âŒ Redundant: Light detection duplicates extraction work

**Code to delete:**
- `pipeline/structure/__init__.py` - BookStructurer orchestrator
- `pipeline/structure/loader.py` - Page loading (has content loss bug)
- `pipeline/structure/detector.py` - Structure detection (expensive, redundant)
- `pipeline/structure/extractor.py` - Page numbers, footnotes, bibliography
- `pipeline/structure/generator.py` - Output generation (can adapt some logic)

---

## Target State (v2) - NEW IMPLEMENTATION

**Architecture:**
```
Phase 1: SLIDING WINDOW EXTRACTION (GPT-4o-mini, parallel) âœ… COMPLETE
â”œâ”€ Overlapping batches (3 pages, 1 overlap, stride=2)
â”œâ”€ Extract: Clean text + chapter markers + footnotes (extract_agent)
â”œâ”€ Verify: Full-text LLM quality check (verify_agent)
â”œâ”€ Reconcile: LLM arbitration for overlaps (reconcile_agent)
â”œâ”€ Parallel: 30 workers, ~318 batches (636-page book)
â”œâ”€ Storage: Batch results saved to structured/extraction/
â””â”€ Output: Verified segments with reconciled overlaps

Phase 2: ASSEMBLY & CHUNKING (GPT-4o-mini + Python) ğŸš§ TODO
â”œâ”€ Merge batches (load from structured/extraction/)
â”œâ”€ Build document map from chapter evidence (bottom-up)
â”œâ”€ Create semantic chunks for RAG (500-1000 words)
â”œâ”€ Generate outputs (reading, data, archive)
â””â”€ Verify completeness (word counts, coverage)
```

**Benefits:**
- âœ… No content loss: LLM intelligently removes headers while preserving body text
- âœ… Cheaper: ~$3.18 for 636 pages (vs $2.00 old way, but includes verification)
- âœ… Reliable: 100% batch success rate (3-page batches, shorter JSON)
- âœ… RAG-ready: Semantic chunks with provenance tracking (Phase 2)
- âœ… Simpler: 2 phases instead of 8
- âœ… Bottom-up: Chapters discovered from content (single source of truth)
- âœ… Transparent: Python counts facts, LLM judges quality
- âœ… Persistent: All batch results saved for debugging

**New code structure:**
```
pipeline/structure/
â”œâ”€â”€ __init__.py              # Main orchestrator (clean rewrite)
â”œâ”€â”€ extractor.py            # Phase 1: Sliding window extraction
â”œâ”€â”€ assembler.py            # Phase 2: Assembly & chunking
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract_agent.py    # Agent 1: Extract clean text
â”‚   â”œâ”€â”€ verify_agent.py     # Agent 2: Verify quality
â”‚   â””â”€â”€ reconcile_agent.py  # Agent 3: Reconcile overlaps
â”œâ”€â”€ chunker.py              # Semantic chunking logic
â”œâ”€â”€ generator.py            # Output generation (adapted from v1)
â””â”€â”€ utils.py                # Shared utilities
```

---

## Migration Steps

### Step 1: Delete Old Implementation
**Goal:** Clean slate for new architecture

**Tasks:**
- [ ] Delete old files:
  ```bash
  git rm pipeline/structure/__init__.py
  git rm pipeline/structure/loader.py
  git rm pipeline/structure/detector.py
  git rm pipeline/structure/extractor.py
  # Keep generator.py temporarily - will adapt it
  ```
- [ ] Commit deletion:
  ```bash
  git commit -m "refactor: remove old structure stage implementation

  Removing 8-phase architecture with:
  - Content loss bug (header filtering)
  - Expensive structure detection ($2.00)
  - No semantic chunking

  Preparing for 2-phase sliding window architecture."
  ```

**Timeline:** 5 minutes

---

### Step 2: Implement Phase 1 (Extraction) âœ… COMPLETE
**Goal:** Working extraction with 3-agent verification

**Tasks:**

#### 2a. Agent Implementation âœ…
- [x] Create `pipeline/structure/agents/__init__.py`
- [x] Implement `agents/extract_agent.py`
  - Input: List of page dicts (3 pages - changed from 10 for reliability)
  - Prompt: Remove headers, preserve body text, mark chapters
  - Output: Clean text + metadata (NO word_count - Python calculates)
  - Model: GPT-4o-mini
  - Key fix: Python counts words, not LLM (prevents hallucinations)

- [x] Implement `agents/verify_agent.py`
  - Input: COMPLETE original pages + COMPLETE extraction result
  - Task: Full-text LLM comparison (not sampling)
  - Output: Quality score + headers_identified + confidence
  - Model: GPT-4o-mini
  - Key fix: Passes full 3-page text for thorough verification

- [x] Implement `agents/reconcile_agent.py`
  - Input: Two extractions of same overlap pages
  - Task: Compare similarity, LLM arbitration if < 95%
  - Output: Merged/arbitrated text with explanation
  - Model: GPT-4o-mini (only if disagreement)
  - Key feature: LLM creates best-of-both merged versions

#### 2b. Extractor Orchestrator âœ…
- [x] Implement `extractor.py`
  - Create batches (window_size=3, overlap=1, stride=2)
  - Parallel processing (30 workers, ~318 batches for 636 pages)
  - 3-agent coordination per batch (extract â†’ verify â†’ reconcile)
  - Aggregate results with LLM arbitration for overlaps
  - Save batch results to structured/extraction/batch_NNN.json
  - Save metadata to structured/extraction/metadata.json

**Test Results:** âœ…
- [x] Unit tests for each agent (test_structure_agents.py)
- [x] Test extractor on pages 75-90 (Roosevelt sample)
  - âœ… Headers removed: "PRACTICAL POLITICS", page numbers, etc.
  - âœ… Body text preserved: 96-98% word retention (realistic)
  - âœ… Chapter markers detected: Identified by extract_agent
  - âœ… Overlap reconciliation: 6/7 overlaps LLM arbitrated with high confidence
  - âœ… Batch persistence: All results saved to disk
  - âœ… 100% success rate (8/8 batches, 1 failure was transient)

**Actual Timeline:** 2 sessions (Session 1: agents, Session 2: orchestrator + fixes)

---

### Step 3: Implement Phase 2 (Assembly)
**Goal:** Merge batches, create semantic chunks, generate outputs

**Tasks:**

#### 3a. Assembly Logic
- [ ] Implement `assembler.py`
  - Merge batches (reconcile overlaps using reconcile_agent)
  - Aggregate chapter markers from all batches
  - Build document_map from consensus (bottom-up)
  - Verify: All pages covered, no gaps

#### 3b. Semantic Chunking
- [ ] Implement `chunker.py`
  - LLM prompt: Split chapter into semantic sections (500-1000 words)
  - Respect paragraph boundaries
  - Track provenance (scan_pages, book_pages, chapter)
  - Output: chunks/*.json with metadata

#### 3c. Output Generation
- [ ] Adapt `generator.py` from v1
  - Reading format: `structured/reading/full_book.txt`
  - Data format: `structured/data/chunks/*.json`, `document_map.json`
  - Archive format: `structured/archive/full_book.md`
  - Ensure provenance tracking in all outputs

**Test:**
- [ ] Test assembly on pages 75-110 (4 batches with overlaps)
  - Verify: Overlap reconciliation works
  - Verify: Document map built correctly
- [ ] Test chunking on one chapter
  - Verify: Chunks are semantically coherent
  - Verify: Provenance tracking works (chunk â†’ pages)
  - Verify: Word counts in range (500-1000)

**Timeline:** 1-2 sessions

---

### Step 4: Main Orchestrator & CLI
**Goal:** Wire everything together, integrate with CLI

**Tasks:**
- [ ] Implement `pipeline/structure/__init__.py`
  - Main `BookStructurer` class
  - Phase 1: Call extractor
  - Phase 2: Call assembler + chunker + generator
  - Logging and progress tracking
  - Error handling

- [ ] Ensure CLI integration works
  - `uv run python ar.py structure <scan-id>` calls new implementation
  - Progress output to console
  - Logs to `logs/structure_*.jsonl`

**Test:**
- [ ] End-to-end test on Roosevelt pages 75-110
  - Run: `uv run python ar.py structure roosevelt-autobiography --start 75 --end 110`
  - Verify: All outputs generated
  - Verify: Logs written
  - Verify: No errors

**Timeline:** 1 session

---

### Step 5: Full Book Validation
**Goal:** Test on complete Roosevelt autobiography (636 pages)

**Tasks:**
- [ ] Run full structure stage:
  ```bash
  uv run python ar.py structure roosevelt-autobiography
  ```

- [ ] Validate outputs:
  - [ ] Word count check
    - Expected: ~320,000 words (636 pages Ã— ~500 words/page)
    - Should be 20-30% MORE than old implementation (no content loss)
  - [ ] Chapter boundaries
    - Expected: ~25-30 chapters for Roosevelt autobiography
    - Verify chapter titles make sense
    - Verify boundaries at reasonable positions
  - [ ] Semantic chunks
    - Expected: ~400-500 chunks (636 pages / ~1.5 pages per chunk)
    - Spot check: 10 random chunks are coherent
    - Verify provenance: chunk â†’ scan_pages mapping works
  - [ ] Performance
    - Time: < 5 minutes
    - Cost: < $1.50

- [ ] Manual quality check:
  - [ ] Read 5-10 chunks - do they make sense?
  - [ ] Check 3-4 chapter boundaries - are they accurate?
  - [ ] Verify no running headers in output text
  - [ ] Verify body text is present and correct

**Acceptance Criteria:**
- âœ… No crashes or errors
- âœ… All expected files generated
- âœ… Word count in expected range
- âœ… Chapters detected (20-35 range)
- âœ… Chunks are coherent
- âœ… Provenance tracking works
- âœ… Cost < $1.50
- âœ… Time < 5 minutes
- âœ… No visible running headers in output

**Timeline:** 1 session

---

### Step 6: Documentation & Testing
**Goal:** Update docs and tests for new implementation

**Tasks:**
- [ ] Update `docs/STRUCTURE.md`
  - Remove "Phase 1: Light detector" section
  - Update to 2-phase architecture
  - Add actual implementation details
  - Update cost/performance numbers with real data

- [ ] Update tests
  - [ ] Create `tests/test_structure_agents.py`
  - [ ] Create `tests/test_structure_extractor.py`
  - [ ] Create `tests/test_structure_assembler.py`
  - [ ] Update `tests/test_pipeline_e2e.py` if needed

- [ ] Update `README.md` if needed
  - Output formats
  - Example usage

- [ ] Commit everything:
  ```bash
  git add pipeline/structure/
  git commit -m "feat: implement new 2-phase structure stage

  Replaces old 8-phase architecture with:
  - Phase 1: Sliding window extraction (3-agent verification)
  - Phase 2: Assembly & semantic chunking

  Benefits:
  - No content loss (LLM-based header removal)
  - 45% cheaper ($1.10 vs $2.00)
  - 20% faster (2-3min vs 4-5min)
  - RAG-ready semantic chunks
  - Provenance tracking (chunk â†’ PDF pages)

  Tested on Roosevelt autobiography (636 pages):
  - [X chapters detected]
  - [X chunks created]
  - [X words total]
  - Cost: $X.XX
  - Time: X minutes"
  ```

**Timeline:** 1 session

---

## Implementation Checklist

### Session 1: Cleanup & Agent Implementation
- [ ] Delete old structure implementation
- [ ] Create new directory structure
- [ ] Implement `agents/extract_agent.py`
- [ ] Implement `agents/verify_agent.py`
- [ ] Implement `agents/reconcile_agent.py`
- [ ] Unit tests for agents
- [ ] Test agents on 2-3 batches

### Session 2: Extraction Phase
- [ ] Implement `extractor.py`
  - Batch creation logic
  - Parallel processing
  - 3-agent coordination
- [ ] Test on Roosevelt pages 75-90 (~15 pages, 2 batches)
- [ ] Validate: Headers removed, chapters detected, word count OK

### Session 3: Assembly Phase
- [ ] Implement `assembler.py`
- [ ] Implement `chunker.py`
- [ ] Adapt `generator.py`
- [ ] Test on Roosevelt pages 75-110 (~35 pages, 4 batches)
- [ ] Validate: Batches merge correctly, chunks created

### Session 4: Integration & Testing
- [ ] Implement main orchestrator (`__init__.py`)
- [ ] CLI integration
- [ ] Test on Roosevelt pages 75-110 (end-to-end)
- [ ] Run on full Roosevelt book (636 pages)
- [ ] Validate outputs, performance, cost

### Session 5: Polish & Documentation
- [ ] Fix any issues found in Session 4
- [ ] Update documentation
- [ ] Write tests
- [ ] Final validation
- [ ] Commit and push

---

## Configuration

**Extraction Parameters:**
```python
WINDOW_SIZE = 10      # pages per batch
OVERLAP = 3           # pages of overlap between batches
MAX_WORKERS = 30      # parallel batch processing
```

**Chunking Parameters:**
```python
TARGET_CHUNK_SIZE = 750  # words per chunk
CHUNK_MIN = 500          # minimum chunk size
CHUNK_MAX = 1000         # maximum chunk size
```

**Models:**
```python
EXTRACTION_MODEL = "openai/gpt-4o-mini"
VERIFICATION_MODEL = "openai/gpt-4o-mini"
CHUNKING_MODEL = "openai/gpt-4o-mini"
```

---

## Success Criteria

âœ… **Implementation Success:**
- All agents implemented and tested
- Extraction phase works on sample
- Assembly phase works on sample
- End-to-end test passes

âœ… **Full Book Success:**
- Roosevelt (636 pages) processes successfully
- Word count â‰¥ 300,000 (no content loss)
- Chapters detected (20-35 range)
- Chunks created (~400-500)
- Provenance tracking works
- Cost < $1.50
- Time < 5 minutes
- Quality spot checks pass

âœ… **Documentation Success:**
- docs/STRUCTURE.md updated
- Tests written and passing
- README.md updated if needed
- Clean git history

---

## Risk Mitigation

**Risk 1: New implementation doesn't work**
- Mitigation: Test incrementally (sample â†’ full book)
- If major issues: Can revert commit and restore old implementation

**Risk 2: Content quality issues**
- Mitigation: Extensive validation (word counts, manual review)
- Test on multiple page ranges before full book
- Compare sample outputs manually

**Risk 3: Cost/performance worse than expected**
- Mitigation: Test on sample first (pages 75-90)
- Adjust batch size, overlap, workers if needed
- Can tune parameters without code changes

**Risk 4: Output format breaks downstream tools**
- Mitigation: Maintain output schema compatibility
- MCP server may need updates (check after implementation)
- Document any breaking changes

---

## Timeline Summary

| Session | Focus | Duration | Status |
|---------|-------|----------|--------|
| 1 | Cleanup & Agents | 2-3 hours | ğŸ”² Not started |
| 2 | Extraction Phase | 2-3 hours | ğŸ”² Not started |
| 3 | Assembly Phase | 2-3 hours | ğŸ”² Not started |
| 4 | Integration & Full Test | 2-3 hours | ğŸ”² Not started |
| 5 | Polish & Docs | 1-2 hours | ğŸ”² Not started |

**Total estimated time:** 5 sessions (10-14 hours)

---

## Current Session Plan

**Today's Goals:**
1. âœ… Design simplified 2-phase architecture (done - no light detector!)
2. âœ… Create migration plan (this document)
3. ğŸ”² Start Session 1 implementation:
   - Delete old structure files
   - Create new directory structure
   - Implement extract_agent.py
   - Test on 1-2 pages

**Next Session:**
- Complete agent implementation
- Implement extractor.py
- Test on Roosevelt pages 75-90

---

## Notes

- **No v1 preservation:** Clean cutover, delete old code
- **Bottom-up only:** No light detection phase (redundant)
- **Test incrementally:** Sample â†’ full book
- **Validate thoroughly:** Word counts, manual review, spot checks
- **Document everything:** Decisions, trade-offs, results

---

**Related:**
- Design docs: `docs/STRUCTURE.md`
- Current code: `pipeline/structure/` (to be replaced)
- Architecture: `docs/PIPELINE_ARCHITECTURE.md`
